x-airflow-common:
  &airflow-common
  # In order to add custom dependencies or upgrade provider distributions you can use your extended image.
  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
  # and uncomment the "build" line below, Then run `docker-compose build` to build the images.
  image: custom-airflow:3.1.6 #${AIRFLOW_IMAGE_NAME:-apache/airflow:3.1.6}
  build:
    context: ./airflow
    dockerfile: Dockerfile
#  env_file:
#    - ${ENV_FILE_PATH:-.env}
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@host.docker.internal:5432/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@host.docker.internal:5432/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@airflow-redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-apiserver:8080/execution/'
    # yamllint disable rule:line-length
    # Use simple http server on scheduler for health checks
    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server
    # yamllint enable rule:line-length
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
    # for other purpose (development, test and especially production usage) build/extend Airflow image.
    #_PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    # The following line can be used to set a custom config file, stored in the local config folder
    AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
  volumes:
    - ${AIRFLOW_PROJ_DIR:-./airflow}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-./airflow}/logs:/opt/airflow/logs
    - ${AIRFLOW_PROJ_DIR:-./airflow}/config:/opt/airflow/config
    - ${AIRFLOW_PROJ_DIR:-./airflow}/plugins:/opt/airflow/plugins
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    airflow-redis:
      condition: service_healthy

services:
  # --- Kafka KRaft controllers ---
  kafka-controller-1:
    image: apache/kafka:4.1.1
    container_name: kafka-controller-1
    command: |
      bash -c "  
      /opt/kafka/bin/kafka-storage.sh format --config /etc/kafka/controller.properties --cluster-id kraft-cluster-0001 --ignore-formatted &&
      /opt/kafka/bin/kafka-server-start.sh /etc/kafka/controller.properties
      "
    environment:
      KAFKA_LOG_DIRS: /var/lib/kafka
    volumes:
      - ./kafka/kraft/controller-1.properties:/etc/kafka/controller.properties
      - kafka-controller-1-data:/var/lib/kafka
      - ./kafka/generated/certs/controller-1/kafka-controller-1-keystore.p12:/etc/kafka/secrets/keystore.p12:ro
      - ./kafka/generated/certs/truststore/kafka-truststore.p12:/etc/kafka/secrets/truststore.p12:ro
    networks: [appnet]

  kafka-controller-2:
    image: apache/kafka:4.1.1
    container_name: kafka-controller-2
    command: |
      bash -c "
      /opt/kafka/bin/kafka-storage.sh format --config /etc/kafka/controller.properties --cluster-id kraft-cluster-0001 --ignore-formatted &&
      /opt/kafka/bin/kafka-server-start.sh /etc/kafka/controller.properties
      "
    environment:
      KAFKA_LOG_DIRS: /var/lib/kafka
    volumes:
      - ./kafka/kraft/controller-2.properties:/etc/kafka/controller.properties
      - kafka-controller-2-data:/var/lib/kafka
      - ./kafka/generated/certs/controller-2/kafka-controller-2-keystore.p12:/etc/kafka/secrets/keystore.p12:ro
      - ./kafka/generated/certs/truststore/kafka-truststore.p12:/etc/kafka/secrets/truststore.p12:ro
    networks: [appnet]

  kafka-controller-3:
    image: apache/kafka:4.1.1
    container_name: kafka-controller-3
    command: |
      bash -c "
      /opt/kafka/bin/kafka-storage.sh format --config /etc/kafka/controller.properties --cluster-id kraft-cluster-0001 --ignore-formatted &&
      /opt/kafka/bin/kafka-server-start.sh /etc/kafka/controller.properties
      "
    environment:
      KAFKA_LOG_DIRS: /var/lib/kafka
    volumes:
      - ./kafka/kraft/controller-3.properties:/etc/kafka/controller.properties
      - kafka-controller-3-data:/var/lib/kafka
      - ./kafka/generated/certs/controller-3/kafka-controller-3-keystore.p12:/etc/kafka/secrets/keystore.p12:ro
      - ./kafka/generated/certs/truststore/kafka-truststore.p12:/etc/kafka/secrets/truststore.p12:ro
    networks: [appnet]

  # --- Kafka KRaft brokers ---
  kafka-broker-1:
    image: apache/kafka:4.1.1
    container_name: kafka-broker-1
    depends_on: [kafka-controller-1, kafka-controller-2, kafka-controller-3]
    ports:
      - "19092:19092"
    command: |
      bash -c "
      /opt/kafka/bin/kafka-storage.sh format --config /etc/kafka/server.properties --cluster-id kraft-cluster-0001 --ignore-formatted &&
      /opt/kafka/bin/kafka-server-start.sh /etc/kafka/server.properties
      "
    environment:
      KAFKA_LOG_DIRS: /var/lib/kafka/data
    volumes:
      - ./kafka/kraft/broker-1.properties:/etc/kafka/server.properties
      - kafka-broker-1-data:/var/lib/kafka
      - ./kafka/generated/certs/broker-1/kafka-broker-1-keystore.p12:/etc/kafka/secrets/keystore.p12:ro
      - ./kafka/generated/certs/truststore/kafka-truststore.p12:/etc/kafka/secrets/truststore.p12:ro
    networks: [appnet]

  kafka-broker-2:
    image: apache/kafka:4.1.1
    container_name: kafka-broker-2
    depends_on: [ kafka-controller-1, kafka-controller-2, kafka-controller-3 ]
    ports:
      - "19093:19092"
    command: |
      bash -c "
      /opt/kafka/bin/kafka-storage.sh format --config /etc/kafka/server.properties --cluster-id kraft-cluster-0001 --ignore-formatted &&
      /opt/kafka/bin/kafka-server-start.sh /etc/kafka/server.properties
      "
    environment:
      KAFKA_LOG_DIRS: /var/lib/kafka/data
    volumes:
      - ./kafka/kraft/broker-2.properties:/etc/kafka/server.properties
      - kafka-broker-2-data:/var/lib/kafka
      - ./kafka/generated/certs/broker-2/kafka-broker-2-keystore.p12:/etc/kafka/secrets/keystore.p12:ro
      - ./kafka/generated/certs/truststore/kafka-truststore.p12:/etc/kafka/secrets/truststore.p12:ro
    networks: [ appnet ]

  kafka-broker-3:
    image: apache/kafka:4.1.1
    container_name: kafka-broker-3
    depends_on: [ kafka-controller-1, kafka-controller-2, kafka-controller-3 ]
    ports:
      - "19094:19092"
    command: |
      bash -c "
      /opt/kafka/bin/kafka-storage.sh format --config /etc/kafka/server.properties --cluster-id kraft-cluster-0001 --ignore-formatted &&
      /opt/kafka/bin/kafka-server-start.sh /etc/kafka/server.properties
      "
    environment:
      KAFKA_LOG_DIRS: /var/lib/kafka/data
    volumes:
      - ./kafka/kraft/broker-3.properties:/etc/kafka/server.properties
      - kafka-broker-3-data:/var/lib/kafka
      - ./kafka/generated/certs/broker-3/kafka-broker-3-keystore.p12:/etc/kafka/secrets/keystore.p12:ro
      - ./kafka/generated/certs/truststore/kafka-truststore.p12:/etc/kafka/secrets/truststore.p12:ro
    networks: [ appnet ]

  # --- Schema Registry (Avro) ---
  kafka-confl-schema-registry:
    image: confluentinc/cp-schema-registry:7.6.1
    container_name: kafka-confl-schema-registry
    depends_on: [kafka-broker-1, kafka-broker-2, kafka-broker-3]
    environment:
      SCHEMA_REGISTRY_HOST_NAME: kafka-confl-schema-registry
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: SSL://kafka-broker-1:9092,SSL://kafka-broker-2:9092,SSL://kafka-broker-3:9092
      SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL: SSL
      SCHEMA_REGISTRY_KAFKASTORE_SSL_KEYSTORE_LOCATION: /etc/schema-registry/secrets/keystore.p12
      SCHEMA_REGISTRY_KAFKASTORE_SSL_KEYSTORE_PASSWORD: ${KAFKA_CONFL_SCHEMA_REGISTRY_KEYSTORE_PASSWORD}
      SCHEMA_REGISTRY_KAFKASTORE_SSL_KEY_PASSWORD: ${KAFKA_CONFL_SCHEMA_REGISTRY_KEYSTORE_PASSWORD}
      SCHEMA_REGISTRY_KAFKASTORE_SSL_TRUSTSTORE_LOCATION: /etc/schema-registry/secrets/truststore.p12
      SCHEMA_REGISTRY_KAFKASTORE_SSL_TRUSTSTORE_PASSWORD: ${TRUSTSTORE_PASSWORD}

      SCHEMA_REGISTRY_AVRO_COMPATIBILITY_LEVEL: BACKWARD
    volumes:
      - ./kafka/generated/certs/confl-schema-registry/kafka-confl-schema-registry-keystore.p12:/etc/schema-registry/secrets/keystore.p12:ro
      - ./kafka/generated/certs/truststore/kafka-truststore.p12:/etc/schema-registry/secrets/truststore.p12:ro
    ports:
      - "9081:8081"
    networks: [appnet]

  ##############################
  # Apicurio Registry (Avro)   #
  ##############################
  apicurio-registry:
    image: apicurio/apicurio-registry:latest-release
    container_name: apicurio-registry
    ports:
      - "9082:8080"    # Apicurio runs on 8080 internally
    environment:
      - QUARKUS_HTTP_PORT=8080
      - APICURIO_STORAGE_KIND=sql
      - APICURIO_STORAGE_SQL_KIND=postgresql
      - APICURIO_DATASOURCE_URL=jdbc:postgresql://host.docker.internal:5432/apicurio
      - APICURIO_DATASOURCE_USERNAME=apicurio
      - APICURIO_DATASOURCE_PASSWORD=secret
    networks: [ appnet ]

  # --- Kafka Connect (Debezium + sinks) ---
  kafka-connect:
    image: quay.io/debezium/connect:2.7
    container_name: kafka-connect
    depends_on:
      - kafka-broker-1
      - kafka-broker-2
      - kafka-broker-3
      - apicurio-registry

    environment:
      # ------------------------------------------------------------
      # Kafka Connect worker SSL configuration (correct prefixes)
      # ------------------------------------------------------------
      BOOTSTRAP_SERVERS: kafka-broker-1:9092,kafka-broker-2:9092,kafka-broker-3:9092

      CONNECT_SECURITY_PROTOCOL: SSL
      CONNECT_SSL_KEYSTORE_LOCATION: /etc/kafka-connect/secrets/keystore.p12
      CONNECT_SSL_KEYSTORE_PASSWORD: ${KAFKA_CONNECT_KEYSTORE_PASSWORD}
      CONNECT_SSL_KEY_PASSWORD: ${KAFKA_CONNECT_KEYSTORE_PASSWORD}
      CONNECT_SSL_KEYSTORE_TYPE: PKCS12

      CONNECT_SSL_TRUSTSTORE_LOCATION: /etc/kafka-connect/secrets/truststore.p12
      CONNECT_SSL_TRUSTSTORE_PASSWORD: ${TRUSTSTORE_PASSWORD}
      CONNECT_SSL_TRUSTSTORE_TYPE: PKCS12

      # Enable hostname verification (your certs are correct)
      CONNECT_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: https

      # ------------------------------------------------------------
      # Internal Kafka Connect topics
      # ------------------------------------------------------------
      GROUP_ID: connect-cluster
      CONFIG_STORAGE_TOPIC: _connect-configs
      OFFSET_STORAGE_TOPIC: _connect-offsets
      STATUS_STORAGE_TOPIC: _connect-status

      CONFIG_STORAGE_REPLICATION_FACTOR: 3
      OFFSET_STORAGE_REPLICATION_FACTOR: 3
      STATUS_STORAGE_REPLICATION_FACTOR: 3

      # ------------------------------------------------------------
      # Apicurio converters
      # ------------------------------------------------------------
      ENABLE_APICURIO_CONVERTERS: "true"

      KEY_CONVERTER: io.apicurio.registry.utils.converter.AvroConverter
      VALUE_CONVERTER: io.apicurio.registry.utils.converter.AvroConverter

      KEY_CONVERTER_APICURIO_REGISTRY_URL: http://apicurio-registry:8080/apis/registry/v2
      VALUE_CONVERTER_APICURIO_REGISTRY_URL: http://apicurio-registry:8080/apis/registry/v2

      KEY_CONVERTER_APICURIO_REGISTRY_AUTO_REGISTER: "true"
      VALUE_CONVERTER_APICURIO_REGISTRY_AUTO_REGISTER: "true"

      KEY_CONVERTER_APICURIO_REGISTRY_FIND_LATEST: "true"
      VALUE_CONVERTER_APICURIO_REGISTRY_FIND_LATEST: "true"

      # ------------------------------------------------------------
      # REST API
      # ------------------------------------------------------------
      REST_ADVERTISED_HOST_NAME: kafka-connect
      REST_PORT: 8083

      # ------------------------------------------------------------
      # Plugins
      # ------------------------------------------------------------
      PLUGIN_PATH: /kafka/connect/plugins
      ERRORS_LOG_ENABLE: "true"
    volumes:
      - ./kafka/generated/connect/plugins:/kafka/connect/plugins
      - ./kafka/generated/certs/connect/kafka-connect-keystore.p12:/etc/kafka-connect/secrets/keystore.p12:ro
      - ./kafka/generated/certs/truststore/kafka-truststore.p12:/etc/kafka-connect/secrets/truststore.p12:ro
    ports:
      - "9083:8083"
    networks:
      - appnet

  kafdrop:
    image: obsidiandynamics/kafdrop:latest
    container_name: kafdrop
    depends_on:
      - kafka-broker-1
      - kafka-broker-2
      - kafka-broker-3
      - kafka-confl-schema-registry
    ports:
      - "9050:9000"   # expose only if you want UI from host
    environment:
      # ------------------------------------------------------------
      # Kafka cluster connection (internal SSL listeners)
      # ------------------------------------------------------------
      KAFKA_BROKERCONNECT: "kafka-broker-1:9092,kafka-broker-2:9092,kafka-broker-3:9092"
      SERVER_PORT: 9000
      SERVER_SERVLET_CONTEXTPATH: "/"

      # ------------------------------------------------------------
      # Schema Registry integration (Confluent)
      # ------------------------------------------------------------
      SCHEMAREGISTRY_CONNECT: "http://kafka-confl-schema-registry:8081"

      KAFKA_PROPERTIES_FILE: /kafdrop/kafka.properties
      KAFKA_TRUSTSTORE_FILE: /etc/kafdrop/secrets/truststore.p12
      KAFKA_KEYSTORE_FILE: /etc/kafdrop/secrets/keystore.p12

      # ------------------------------------------------------------
      # JVM tuning (optional)
      # ------------------------------------------------------------
      JVM_OPTS: "-Xms512M -Xmx2G"

    volumes:
      - ./kafka/kafdrop/kafka.properties:/kafdrop/kafka.properties:ro
      - ./kafka/generated/certs/kafdrop/kafdrop-keystore.p12:/etc/kafdrop/secrets/keystore.p12:ro
      - ./kafka/generated/certs/truststore/kafka-truststore.p12:/etc/kafdrop/secrets/truststore.p12:ro

    restart: unless-stopped
    networks:
      - appnet

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "9180:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: kraft-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka-broker-1:9092,kafka-broker-2:9092,kafka-broker-3:9092

      # SSL / mTLS
      KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: SSL
      KAFKA_CLUSTERS_0_PROPERTIES_SSL_TRUSTSTORE_LOCATION: /etc/kafka-ui/truststore.p12
      KAFKA_CLUSTERS_0_PROPERTIES_SSL_TRUSTSTORE_PASSWORD: truststore-pass_length_at_least_32_chars_suggested_around_256_but_is_unlimited_should_be_with_high_entropy_random_accept_utf8
      KAFKA_CLUSTERS_0_PROPERTIES_SSL_KEYSTORE_LOCATION: /etc/kafka-ui/keystore.p12
      KAFKA_CLUSTERS_0_PROPERTIES_SSL_KEYSTORE_PASSWORD: kafka_ui-keystore_pass_and_key_pass_same_as_one_length_at_least_32_chars_suggested_around_256_but_is_unlimited_should_be_with_high_entropy_random_accept_utf8
      KAFKA_CLUSTERS_0_PROPERTIES_SSL_KEY_PASSWORD: kafka_ui-keystore_pass_and_key_pass_same_as_one_length_at_least_32_chars_suggested_around_256_but_is_unlimited_should_be_with_high_entropy_random_accept_utf8

    volumes:
      - ./kafka/generated/certs/kafka-ui/kafka-ui-keystore.p12:/etc/kafka-ui/keystore.p12:ro
      - ./kafka/generated/certs/truststore/kafka-truststore.p12:/etc/kafka-ui/truststore.p12:ro
    networks:
      - appnet


  # --- Elasticsearch (community single-node) ---
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.15.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ports:
      - "9200:9200"
    networks: [appnet]

  # --- OpenSearch (single-node) + Dashboards ---
  opensearch:
    image: opensearchproject/opensearch:2.14.0
    container_name: opensearch
    environment:
      - discovery.type=single-node
      - OPENSEARCH_JAVA_OPTS=-Xms1g -Xmx1g

      # Security ON
      - plugins.security.disabled=false

      # Admin password
      - OPENSEARCH_INITIAL_ADMIN_PASSWORD=${OPENSEARCH_ADMIN_PASSWORD}

      # === HTTP (REST) TLS ===
      - plugins.security.ssl.http.enabled=true
      - plugins.security.ssl.http.pemcert_filepath=/usr/share/opensearch/config/certs/opensearch/opensearch.crt
      - plugins.security.ssl.http.pemkey_filepath=/usr/share/opensearch/config/certs/opensearch/opensearch.key
      - plugins.security.ssl.http.pemtrustedcas_filepath=/usr/share/opensearch/config/certs/ca/root-os-ca.crt

      # === Transport (cluster) TLS ===
      - plugins.security.ssl.transport.pemcert_filepath=/usr/share/opensearch/config/certs/opensearch/opensearch-transport.crt
      - plugins.security.ssl.transport.pemkey_filepath=/usr/share/opensearch/config/certs/opensearch/opensearch-transport.key
      - plugins.security.ssl.transport.pemtrustedcas_filepath=/usr/share/opensearch/config/certs/ca/root-os-ca.crt
      - plugins.security.ssl.transport.enforce_hostname_verification=true
      - plugins.security.ssl.transport.resolve_hostname=true

      # Optional: strict security config
      - plugins.security.allow_default_init_securityindex=true # Should be false for prod.
      - plugins.security.audit.type=internal_opensearch

    volumes:
      - ./opensearch/generated/certs/ca:/usr/share/opensearch/config/certs/ca:ro
      - ./opensearch/generated/certs/opensearch:/usr/share/opensearch/config/certs/opensearch:ro
      - opensearch-data:/usr/share/opensearch/data

    ports:
      - "9201:9200"
    networks: [ appnet ]

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.14.0
    container_name: opensearch-dashboards
    environment:
      # Connect to OpenSearch over HTTPS
      - OPENSEARCH_HOSTS=["https://opensearch:9200"]

      # Security ON
      - OPENSEARCH_SECURITY_ENABLED=true

      # Trust OpenSearch's CA
      - OPENSEARCH_SSL_VERIFICATIONMODE=certificate
      - OPENSEARCH_SSL_CERTIFICATEAUTHORITIES=/usr/share/opensearch-dashboards/config/certs/ca/root-os-ca.crt

      # Optional: enable HTTPS for Dashboards itself
      - SERVER_SSL_ENABLED=true
      - SERVER_SSL_CERTIFICATE=/usr/share/opensearch-dashboards/config/certs/opensearch-dashboards/opensearch-dashboards.crt
      - SERVER_SSL_KEY=/usr/share/opensearch-dashboards/config/certs/opensearch-dashboards/opensearch-dashboards.key

    volumes:
      - ./opensearch/generated/certs/ca:/usr/share/opensearch-dashboards/config/certs/ca:ro
      - ./opensearch/generated/certs/opensearch-dashboards:/usr/share/opensearch-dashboards/config/certs/opensearch-dashboards:ro

    ports:
      - "5601:5601"
    depends_on: [ opensearch ]
    networks: [ appnet ]

  # ============================
  # MinIO (S3-compatible storage)
  # ============================
  minio:
    image: minio/minio:latest
    container_name: minio
    command: server /data1 /data2 /data3 /data4 --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin123
    volumes:
      - minio-data1:/data1
      - minio-data2:/data2
      - minio-data3:/data3
      - minio-data4:/data4
    ports:
      - "9000:9000"
      - "9001:9001"
    restart: unless-stopped
    networks: [appnet]

  # ============================
  # Iceberg REST Catalog
  # ============================
  iceberg-rest:
    image: tabulario/iceberg-rest:latest
    container_name: iceberg-rest
    environment:
      CATALOG_WAREHOUSE: s3a://lakehouse/
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_ACCESS_KEY_ID: minioadmin
      CATALOG_S3_SECRET_ACCESS_KEY: minioadmin123
      CATALOG_S3_PATH_STYLE_ACCESS: "true"
    ports:
      - "8181:8181"
    depends_on:
      - minio
    restart: unless-stopped
    networks: [appnet]

  # ============================
  # Spark Master
  # ============================
  spark-master:
    image: apache/spark:4.1.1
    container_name: spark-master
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
      --host spark-master --port 7077 --webui-port 8088
    environment:
      SPARK_NO_DAEMONIZE: "true"
    ports:
      #- "7077:7077"
      - "9088:8088"
    volumes:
      - spark-conf:/opt/spark/conf
      - spark-jars:/opt/spark/jars
      - spark-history:/opt/spark/history
    restart: unless-stopped
    networks: [appnet]

  # ============================
  # Spark Worker
  # ============================
  spark-worker-1:
    image: apache/spark:4.1.1
    container_name: spark-worker-1
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
    environment:
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 4G
      SPARK_NO_DAEMONIZE: "true"
    depends_on:
      - spark-master
    volumes:
      - spark-conf:/opt/spark/conf
      - spark-jars:/opt/spark/jars
      - spark-history:/opt/spark/history
    restart: unless-stopped
    networks: [appnet]

  # ============================
  # Spark History Server
  # ============================
  spark-history:
    image: apache/spark:4.1.1
    container_name: spark-history
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer
    environment:
      SPARK_HISTORY_OPTS: "-Dspark.history.fs.logDirectory=/opt/spark/history"
      SPARK_NO_DAEMONIZE: "true"
    ports:
      - "18080:18080"
    volumes:
      - spark-history:/opt/spark/history
    restart: unless-stopped
    networks: [appnet]

  flink-jobmanager:
    image: apache/flink:2.2
    container_name: flink-jobmanager
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 4
        state.backend: rocksdb
        state.checkpoints.dir: file:///flink-checkpoints
        state.savepoints.dir: file:///flink-savepoints
    ports:
      - "9084:8081"   # Flink UI
    volumes:
      - flink-checkpoints:/flink-checkpoints
      - flink-savepoints:/flink-savepoints
    depends_on:
      - kafka-broker-1
      - minio
    networks:
      - appnet

  flink-taskmanager-1:
    image: apache/flink:2.2
    container_name: flink-taskmanager-1
    command: taskmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 4
    depends_on:
      - flink-jobmanager
    volumes:
      - flink-checkpoints:/flink-checkpoints
      - flink-savepoints:/flink-savepoints
    networks:
      - appnet

  flink-taskmanager-2:
    image: apache/flink:2.2
    container_name: flink-taskmanager-2
    command: taskmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 4
    depends_on:
      - flink-jobmanager
    volumes:
      - flink-checkpoints:/flink-checkpoints
      - flink-savepoints:/flink-savepoints
    networks:
      - appnet

  airflow-redis:
    image: redis:7.4.7-bookworm
    expose:
      - 6379
    container_name: airflow-redis
    #command: [ "redis-server", "--appendonly", "yes" ]
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    networks:
      - appnet

  airflow-apiserver:
    <<: *airflow-common
    container_name: airflow-apiserver
    command: api-server
    ports:
      - "9085:8080"
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8080/api/v2/version" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - appnet

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    command: scheduler
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8974/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - appnet

  airflow-dag-processor:
    <<: *airflow-common
    container_name: airflow-dag-processor
    command: dag-processor
    healthcheck:
      test: [ "CMD-SHELL", 'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"' ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - appnet

  airflow-worker:
    <<: *airflow-common
    container_name: airflow-worker
    command: celery worker
    healthcheck:
      # yamllint disable rule:line-length
      test: ["CMD-SHELL", 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      # Required to handle warm shutdown of the celery workers properly
      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
      DUMB_INIT_SETSID: "0"
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-apiserver:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    networks:
      - appnet

  airflow-triggerer:
    <<: *airflow-common
    container_name: airflow-triggerer
    command: triggerer
    healthcheck:
      test: [ "CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"' ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - appnet

  airflow-init:
    <<: *airflow-common
    container_name: airflow-init
    entrypoint: /bin/bash
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
          echo
          export AIRFLOW_UID=$$(id -u)
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
          echo
        fi
        echo
        echo "Creating missing opt dirs if missing:"
        echo
        mkdir -v -p /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Airflow version:"
        /entrypoint airflow version
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Running airflow config list to create default config file if missing."
        echo
        /entrypoint airflow config list >/dev/null
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Change ownership of files in /opt/airflow to ${AIRFLOW_UID}:0"
        echo
        chown -R "${AIRFLOW_UID}:0" /opt/airflow/
        echo
        echo "Change ownership of files in shared volumes to ${AIRFLOW_UID}:0"
        echo
        chown -v -R "${AIRFLOW_UID}:0" /opt/airflow/{logs,dags,plugins,config}
        echo
        echo "Files in shared volumes:"
        echo
        ls -la /opt/airflow/{logs,dags,plugins,config}

    # yamllint enable rule:line-length
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      #_PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"
    networks:
      - appnet

  airflow-cli:
    <<: *airflow-common
    container_name: airflow-cli
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: "0"
    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
    command:
      - bash
      - -c
      - airflow
    depends_on:
      <<: *airflow-common-depends-on
    networks:
      - appnet

  # You can enable flower by adding "--profile flower" option e.g. docker-compose --profile flower up
  # or by explicitly targeted on the command line e.g. docker-compose up flower.
  # See: https://docs.docker.com/compose/profiles/
  flower:
    <<: *airflow-common
    container_name: flower
    command: celery flower
    profiles:
      - flower
#    environment:
#      CELERY_BROKER_URL: redis://airflow-redis:6379/0

    ports:
      - "9555:5555"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
    networks:
      - appnet

  nginx:
    image: nginx:stable-alpine
    container_name: nginx_proxy
    restart: always

    ports:
      - "80:80"
      - "443:443"

    volumes:
      # Your custom nginx.conf
      - ./nginx/conf/nginx.conf:/etc/nginx/nginx.conf:ro

      # TLS certificates
      - ./nginx/conf/certs/server.crt:/etc/nginx/certs/server.crt:ro
      - ./nginx/conf/certs/server.key:/etc/nginx/certs/server.key:ro

      # Optional: static site or upstream configs
      # - ./html:/usr/share/nginx/html:ro
      # - ./conf.d:/etc/nginx/conf.d:ro

      # Logs (optional but useful)
      - ./nginx/logs:/var/log/nginx

    # Optional: healthcheck for production
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
      - appnet

  keycloak:
    build:
      context: ./keycloak
      dockerfile: Dockerfile
      args:
        KC_HTTP_RELATIVE_PATH: /iam-service
        KC_HTTPS_ENABLED: "true"
        KC_HTTPS_CERTIFICATE_FILE: /opt/keycloak/conf/keycloak.crt
        KC_HTTPS_CERTIFICATE_KEY_FILE: /opt/keycloak/conf/keycloak.key
        KC_HTTP_ENABLED: "false"
        KC_HOSTNAME_STRICT: "true"
        #KC_HOSTNAME_STRICT_HTTPS: "true"
        KC_DB: postgres
    container_name: keycloak
    restart: always
    environment:
      KC_BOOTSTRAP_ADMIN_USERNAME: ${KC_BOOTSTRAP_ADMIN_USERNAME}
      KC_BOOTSTRAP_ADMIN_PASSWORD: ${KC_BOOTSTRAP_ADMIN_PASSWORD}

      KC_DB: postgres
      KC_DB_USERNAME: ${KC_DB_USERNAME}
      KC_DB_PASSWORD: ${KC_DB_PASSWORD}
      KC_DB_URL: jdbc:postgresql://host.docker.internal:5432/keycloak

      KC_HOSTNAME: localhost
      #KC_HOSTNAME_STRICT: "true"
      #KC_HOSTNAME_STRICT_HTTPS: "true"
      KC_PROXY_HEADERS: forwarded

      #KC_HTTPS_ENABLED: "true"
      #KC_HTTPS_CERTIFICATE_FILE: /opt/keycloak/conf/keycloak.crt
      #KC_HTTPS_CERTIFICATE_KEY_FILE: /opt/keycloak/conf/keycloak.key
      #KC_HTTP_ENABLED: "false"

      KC_FRONTEND_URL: https://localhost/iam-service/
      KC_HTTP_RELATIVE_PATH: /iam-service

    ports:
      - "8443:8443"
    volumes:
      # Keycloak config
      - ./keycloak/conf/keycloak.conf:/opt/keycloak/conf/keycloak.conf:ro
      # Certificates (if Keycloak handles HTTPS)
      - ./keycloak/generated/certs/keycloak/keycloak.crt:/opt/keycloak/conf/keycloak.crt:ro
      - ./keycloak/generated/certs/keycloak/keycloak.key:/opt/keycloak/conf/keycloak.key:ro
      # Custom providers (JARs)
      - ./keycloak/providers:/opt/keycloak/providers
      # Custom themes
      - ./keycloak/themes:/opt/keycloak/themes
    networks:
      - appnet

  # --- Spring Boot search API (JDK 25) ---
  catalog-service:
    build:
      context: ../catalog-service
      dockerfile: Dockerfile
    container_name: catalog-service
    environment:
      SPRING_PROFILES_ACTIVE: docker
    depends_on: [elasticsearch, opensearch]
    ports:
      - "8080:8080"
    networks: [appnet]

networks:
  appnet: {}

volumes:
  kafka-controller-1-data: {}
  kafka-controller-2-data: {}
  kafka-controller-3-data: {}
  kafka-broker-1-data: {}
  kafka-broker-2-data: {}
  kafka-broker-3-data: {}
  spark-conf: {}
  spark-jars: {}
  spark-history: {}
  flink-checkpoints: {}
  flink-savepoints: {}
  opensearch-data: {}
  minio-data1: {}
  minio-data2: {}
  minio-data3: {}
  minio-data4: {}
